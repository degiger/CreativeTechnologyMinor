# CreativeTechnologyMinor - DJ SET "mami tereza"
Doku der Arbeit fÃ¼r Creative Technology von Dario Luciano Giger und Nora-Lee RÃ¼ttimann

# mami tereza â€“ live DJ set mit visuals

## Projektbeschreibung

Im Rahmen des Minors **Creative Technology** haben wir zum ersten Mal mit **TouchDesigner** gearbeitet. Das Tool hat uns direkt fasziniert â€“ besonders die Idee, visuelle Designs zu erstellen, die live auf Musik reagieren. Schnell war klar: Wir wollen ein audioreaktives Visual-Set gestalten.

Zusammen mit der DJane unseres Vertrauens, **mami tereza**, haben wir ein DJ-Set geplant, aufgenommen und mit interaktiven Visuals ergÃ¤nzt. Gemeinsam wurde eine passende Trackliste zusammengestellt, und parallel dazu entwickelten wir visuelle Elemente, die in Echtzeit auf die Musik reagieren.

## Tools & Ressourcen

- [TouchDesigner](https://derivative.ca/)
- [Audacity](https://www.audacityteam.org/)
- [YouTube-Tutorials](https://www.youtube.com):
  - von **elekktronaut**
- UnterstÃ¼tzung durch:
  - **ChatGPT** fÃ¼r technische Hilfe und Debugging

## Einarbeitung & Lernprozess

Da es kaum strukturierte EinfÃ¼hrung oder Betreuung seitens der Hochschule gab, mussten wir uns praktisch alles selbst beibringen. Der Lernprozess war dementsprechend langwierig, aber auch sehr intensiv. Mit Geduld, YouTube-Tutorials, Foren und ChatGPT konnten wir uns Schritt fÃ¼r Schritt in TouchDesigner einarbeiten. 

Auch wenn wir den eigenstÃ¤ndigen Lernprozess grundsÃ¤tzlich schÃ¤tzen, empfinden wir die mangelnde Begleitung â€“ wie sie an der **FHGR** leider hÃ¤ufiger vorkommt â€“ als klaren Schwachpunkt. Eine fundierte EinfÃ¼hrung hÃ¤tte vieles erleichtert und uns schneller zu kreativeren Ergebnissen gefÃ¼hrt.

## Umsetzung & Technische Herausforderungen

Die VerknÃ¼pfung der Visuals mit Audio-Daten stellte sich als wesentlich komplexer heraus als erwartet. Die Konzeption der Visuals war kreativ spannend, aber die Umsetzung der Reaktion auf Audiofrequenzen â€“ insbesondere durch Snare- und Kick-Detection â€“ erforderte viel technisches VerstÃ¤ndnis und Feintuning.

UrsprÃ¼nglich wollten wir das Set mit CDJs und einem Mixer aufnehmen. Allerdings lieÃŸ sich dieses Audio-Setup nicht wie geplant gleichzeitig mit **Audacity** und **TouchDesigner** verbinden. Daher sind wir kurzfristig auf einen **Controller** umgestiegen. Zwar lieÃŸ sich der Sound so problemlos aufnehmen, doch die ReaktivitÃ¤t der Visuals auf die Musik war dadurch leider eingeschrÃ¤nkt. Die direkte Verbindung zwischen Audio-Input und Visuals hat unter der technischen Limitierung gelitten.

## Ergebnis

Vorort kÃ¼mmerte sich Dario Giger um die Videoaufnahmen, wÃ¤hrend Nora RÃ¼timann Setdesign, Visuals und Projektion Ã¼bernahm. Das Ergebnis ist ein stimmiges audiovisuelles Set, bei dem Bild und Ton auf Ã¤sthetische Weise miteinander verschmelzen â€“ auch wenn es technisch nicht durchgehend so reaktiv wurde wie ursprÃ¼nglich geplant.

## Fazit

Das Projekt war aufwendig, aber extrem lehrreich. Wir haben unsere FÃ¤higkeiten in der kreativen Arbeit mit Echtzeit-Visuals deutlich erweitert und gelernt, mit neuen Tools, unerwarteten Problemen und limitierten Ressourcen umzugehen. Besonders im Bereich der visuellen Gestaltung konnten wir unsere StÃ¤rken einsetzen und weiterentwickeln.

TouchDesigner hat uns gezeigt, wie viel kreatives Potenzial in der Verbindung von Klang und Bild steckt â€“ und wir sind motiviert, diesen Weg weiterzuverfolgen. Trotz aller Stolpersteine sind wir mit dem Resultat sehr zufrieden und stolz auf das, was wir eigenstÃ¤ndig auf die Beine gestellt haben.

# ğŸ§ Audio-Visualisierung mit Blender â€“ Projektdokumentation (Art Installation)

In diesem Projekt haben wir untersucht, wie Audio-Input in Blender genutzt werden kann, um visuelle Effekte in Echtzeit fÃ¼r eine Art-Installation zu steuern. Die Installation basiert auf synchronisierten 3D-Visuals, die auf musikalische Elemente reagieren.

## ğŸ”§ Voraussetzungen

- Blender (mit aktiviertem Python-Scripting)
- Ein Audiofile oder ein externes Audio-Device
- Grundkenntnisse in Node-basiertem Arbeiten (Geometry Nodes, Shader Nodes etc.)

---

## ğŸ“ Schritt-fÃ¼r-Schritt-Dokumentation

### 1. ğŸµ Vorbereitung des Audio-Inputs

- Wir haben zu Beginn ein fixes Audiofile verwendet, um unser Setup zu testen.
- WÃ¤hrend des Live-Sets sind wir dann auf ein externes Audio-Device umgestiegen.

---

### 2. ğŸ” Analyse der Audiodaten

Wir haben drei Hauptbereiche zur Analyse des Audiosignals eingerichtet:

- **Kick-Detection**: zur Erkennung der Bassdrums (hohe Pegel).
- **Snare-Detection**: zur Erkennung mittlerer Frequenzimpulse.
- **Speed-Detection**: zur Messung der Geschwindigkeit (Tempo) des Tracks.

#### Aufbau:

- Jeder dieser Bereiche war als eigener Ordner bzw. Subnetz im Node-Netzwerk organisiert.
- Ãœber `In`-Nodes haben wir Daten aus dem Ã¼bergeordneten Node-Baum eingebunden.
- Die Analyse erfolgte Ã¼ber `Audio Aspect Analyze Trail`-Methoden.
- Die Ergebnisse wurden mit `Out`-Nodes zurÃ¼ck ins Hauptnetzwerk gefÃ¼hrt.

---

### 3. ğŸš Visuelle Steuerung Ã¼ber Cues

- Wir haben `Cues` definiert, um visuelle AblÃ¤ufe synchron zur Musik zu steuern.
- Beispiel: Die Bewegung von Ballerinas war an den Beat bzw. die Musikgeschwindigkeit gekoppelt.

---

### 4. ğŸŒ€ Umsetzung der Visuals

- Insgesamt haben wir sechs verschiedene Visuals umgesetzt.
- FÃ¼r dynamische Farb- und Bewegungseffekte haben wir intensiv mit **Noise**-Patterns und **Loops** gearbeitet.
- Python-Skripte wurden verwendet, um Noise-Werte dynamisch zu animieren, z.â€¯B.:

### 5. ğŸ© Beispielvisual: Donut mit Text

- Wir haben einen `Torus` (Donut) als zentrales 3D-Objekt verwendet.
- Darauf wurde ein Textobjekt (z.â€¯B. â€Mami Teresaâ€œ) platziert.
- Ãœber `SOPs` (Surface Operators) haben wir die Geometrie verÃ¤ndert.
- Licht, Kamera und Animationen wurden integriert und durch Audiosignale gesteuert.
- Python-Skripte wurden verwendet, um Noise-Werte dynamisch zu animieren, z.â€¯B.:

  ```python
  noise_value = 0.5  # Geringe Bewegungsgeschwindigkeit
